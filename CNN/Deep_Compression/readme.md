# 性能提升方法
    1. 小模型 mobilenet , 更精细模型的设计，紧致网络设计
       mobilenet squeezenet shufflenet 
       
    2. 模型压缩：参数稀疏、剪裁、量化、分解。
    
    3. 高性能计算。
       腾讯 ncnn

# 模型压缩

[DeepCompression-caffe](https://github.com/Ewenwan/DeepCompression-caffe)


## 为什么要压缩网络？
    做过深度学习的应该都知道，NN大法确实效果很赞，
    在各个领域轻松碾压传统算法，
    不过真正用到实际项目中却会有很大的问题：

    计算量非常巨大；
    模型特别吃内存；
    
    这两个原因，使得很难把NN大法应用到嵌入式系统中去，
    因为嵌入式系统资源有限，而NN模型动不动就好几百兆。
    所以，计算量和内存的问题是作者的motivation；

## 如何压缩？
      论文题目已经一句话概括了：
        Prunes the network：只保留一些重要的连接；
        Quantize the weights：通过权值量化来共享一些weights；
        Huffman coding：通过霍夫曼编码进一步压缩；
        
## 效果如何？
    Pruning：把连接数减少到原来的 1/13~1/9； 
    Quantization：每一个连接从原来的 32bits 减少到 5bits；

## 最终效果： 
    - 把AlextNet压缩了35倍，从 240MB，减小到 6.9MB； 
    - 把VGG-16压缩了49北，从 552MB 减小到 11.3MB； 
    - 计算速度是原来的3~4倍，能源消耗是原来的3~7倍；

## 网络压缩(network compression)
    尽管深度神经网络取得了优异的性能，
    但巨大的计算和存储开销成为其部署在实际应用中的挑战。
    有研究表明，神经网络中的参数存在大量的冗余。
    因此，有许多工作致力于在保证准确率的同时降低网路复杂度。
### 0、训练时对参数的更新进行限制，使其趋向于稀疏.
        核参数稀疏： 是在训练过程中，对参数的更新进行限制，使其趋向于稀疏.
                    对于稀疏矩阵，可以使用更加紧致的存储方式，如CSC，
                    但是使用稀疏矩阵操作在硬件平台上运算效率不高，
                    容易受到带宽的影响，因此加速并不明显。 

        在训练过程中，对权重的更新加以正则项进行诱导，使其更加稀疏，使大部分的权值都为0。
        http://papers.nips.cc/paper/6504-learning-structured-sparsity-in-deep-neural-networks.pdf
        
        动态的模型裁剪方法
        https://arxiv.org/pdf/1608.04493.pdf
        
        包括以下两个过程：pruning和splicing，其中pruning就是将认为不中要的weight裁掉，
        但是往往无法直观的判断哪些weight是否重要，
        因此在这里增加了一个splicing的过程，
        将哪些重要的被裁掉的weight再恢复回来，
        类似于一种外科手术的过程，将重要的结构修补回来。
        作者通过在W上增加一个T来实现，T为一个2值矩阵，起到的相当于一个mask的功能，
        当某个位置为1时，将该位置的weight保留，为0时，裁剪。
        在训练过程中通过一个可学习mask将weight中真正不重要的值剔除，从而使得weight变稀疏。
        
        特点：
             核的稀疏化可能需要一些稀疏计算库的支持，其加速的效果可能受到带宽、稀疏度等很多因素的制约；

### 1、低秩近似 （低秩分解 Low Rank Expansion）
     用低秩矩阵近似原有权重矩阵。
     例如，可以用SVD得到原矩阵的最优低秩近似，
     或用Toeplitz矩阵配合Krylov分解近似原矩阵。

        SVD分解：
        全连接层的权重矩阵记作 W∈Rm×n ,首先对 W 进行 SVD 分解，如下：

        W=USV转置
        为了能够用两个较小的矩阵来表示 W ，我们可以取奇异值的前 K 个分量。
        于是，W可以通过下式重建：

        W^=U^S^V^T,其中U^∈Rm×kV^∈Rn×k
        我们唯一需要保存的就是3个比较小的矩阵 U,S,V ,我们可以简单算一下压缩比为 mn/k(m+n+1)


    矩阵的秩概念上就是线性独立的纵列（或者横列）的最大数目。
    行秩和列秩在线性代数中可以证明是相等的，例如：
        3*3的矩阵如下，则 行秩==列秩==秩==3
        1 2 3
        4 5 6
        7 8 9

        1*3的矩阵如下，则 行秩==列址==秩==1
        [1 2 3] 
        3*1的矩阵如下，则 行秩==列址==秩==1
        [1] [2] [3] 
        
    低秩分解，这个名字虽然唬人，
    实际上就是把较大的卷积核分解为两个级联的行卷积核和列卷积核。
    常见的就是一个3*3的卷积层，替换为一个3*1的卷积层加上一个1*3的卷积核。
    容易计算得，
    一个特征图10*10，经过3*3卷积核后得到8*8的特征图输出，
    而替换为低秩后，
    则先得到10*8的特征图然后再得到8*8的特征图。    
    
    
    另外现在越来越多网络中采用1×1的卷积，
    而这种小的卷积使用矩阵分解的方法很难实现网络加速和压缩。
    
    
### 2、剪枝(pruning) 在训练结束后，可以将一些不重要的神经元连接
    结构化Pruning，Filter Pruning，梯度Pruning等方法
    
    (可用权重数值大小衡量配合损失函数中的稀疏约束)或整个滤波器去除，
    之后进行若干轮微调。实际运行中，神经元连接级别的剪枝会
    使结果变得稀疏，
    不利于缓存优化和内存访问，有的需要专门设计配套的运行库。
    相比之下，滤波器级别的剪枝可直接运行在现有的运行库下，
    而滤波器级别的剪枝的关键是如何衡量滤波器的重要程度。
    例如，可用卷积结果的稀疏程度、该滤波器对损失函数的影响、
    或卷积结果对下一层结果的影响来衡量。
    
    特别地，由于计算稀疏矩阵在CPU和GPU上都有特定的方法，所以前向计算也需要对一些部分进行代码修改。
    GPU上计算稀疏需要调用cuSPARSE库，
    而CPU上计算稀疏需要mkl_sparse之类的库去优化稀疏矩阵的计算，
    否则达不到加速效果.
    
    剪枝方法基本流程如下：
        1. 正常流程训练一个神经网络。以CAFFE为例，就是普普通通地训练出一个caffemodel。
        2. 确定一个需要剪枝的层，一般为全连接层，设定一个裁剪阈值或者比例。
            实现上，通过修改代码加入一个与参数矩阵尺寸一致的mask矩阵。
            mask矩阵中只有0和1，实际上是用于重新训练的网络。
        3. 重新训练微调，参数在计算的时候先乘以该mask，则mask位为1的参数值将继续训练通过BP调整，
           而mask位为0的部分因为输出始终为0则不对后续部分产生影响。
        4. 输出模型参数储存的时候，因为有大量的稀疏，所以需要重新定义储存的数据结构，
           仅储存非零值以及其矩阵位置。重新读取模型参数的时候，就可以还原矩阵。


### 3、量化(quantization)。对权重数值进行聚类，
    量化的思想非常简单。
    CNN参数中数值分布在参数空间，
    通过一定的划分方法，
    总是可以划分称为k个类别。
    然后通过储存这k个类别的中心值或者映射值从而压缩网络的储存。

    量化可以分为
    Low-Bit Quantization(低比特量化)、
    Quantization for General Training Acceleration(总体训练加速量化)和
    Gradient Quantization for Distributed Training(分布式训练梯度量化)。

    由于在量化、特别是低比特量化实现过程中，
    由于量化函数的不连续性，在计算梯度的时候会产生一定的困难。
    对此，阿里巴巴冷聪等人把低比特量化转化成ADMM可优化的目标函数，从而由ADMM来优化。

    从另一个角度思考这个问题，使用哈希把二值权重量化，再通过哈希求解.

    用聚类中心数值代替原权重数值，配合Huffman编码，
    具体可包括标量量化或乘积量化。
    但如果只考虑权重自身，容易造成量化误差很低，
    但分类误差很高的情况。
    因此，Quantized CNN优化目标是重构误差最小化。
    此外，可以利用哈希进行编码，
    即被映射到同一个哈希桶中的权重共享同一个参数值。

    聚类例子：
        例如下面这个矩阵。

        1.2  1.3  6.1
        0.9  0.7  6.9
        -1.0 -0.9 1.0
        设定类别数k=3，通过kmeans聚类。得到：
        A类中心： 1.0 , 映射下标： 1
        B类中心： 6.5 , 映射下标： 2
        C类中心： -0.95 , 映射下标： 3

        所以储存矩阵可以变换为(距离哪个中心近，就用中心的下标替换)：
        1  1  2
        1  1  2
        3  3  1
        当然，论文还提出需要对量化后的值进行重训练，挽回一点丢失的识别率 
        基本上所有压缩方法都有损，所以重训练还是比较必要的。

    Huffman编码笔者已经不太记得了，好像就是高频值用更少的字符储存，低频则用更多。

### 4、降低数据数值范围。 其实也可以算作量化
    默认情况下数据是单精度浮点数，占32位。
    有研究发现，改用半精度浮点数(16位)
    几乎不会影响性能。谷歌TPU使用8位整型来
    表示数据。极端情况是数值范围为二值
    或三值(0/1或-1/0/1)，
    这样仅用位运算即可快速完成所有计算，
    但如何对二值或三值网络进行训练是一个关键。
    通常做法是网络前馈过程为二值或三值，
    梯度更新过程为实数值。

### 5、迁移学习 Knowledge Distillation
    在Knowledge Distillation中有两个关键的问题，
    一是如何定义知识，
    二是使用什么损失函数来度量student网络和teacher 网络之间的相似度。
    
    通过训练一个更大的神经网络模型，再逐步剪枝得到的小模型取得的结果要比直接训练这样一个小模型的结果好得多。
    
## 已训练好的模型上做裁剪
    这种就是在训练好的模型上做一些修改，
    然后在fine-tuning到原来的准确率，
    主要有一些方法：
    1、剪枝：神经网络是由一层一层的节点通过边连接，每个边上会有权重，
            所谓剪枝，就是当我们发现某些边上的权重很小，
            可以认为这样的边不重要，进而可以去掉这些边。
            在训练的过程中，在训练完大模型之后，
            看看哪些边的权值比较小，把这些边去掉，然后继续训练模型；
    2、权值共享：就是让一些边共用一个权值，达到缩减参数个数的目的。
                假设相邻两层之间是全连接，每层有1000个节点，
                那么这两层之间就有1000*1000=100万个权重参数。
                可以将这一百万个权值做聚类，利用每一类的均值代替这一类中的每个权值大小，
                这样同属于一类的很多边共享相同的权值，假设把一百万个权值聚成一千类，则可以把参数个数从一百万降到一千个。
    3、量化：一般而言，神经网络模型的参数都是用的32bit长度的浮点型数表示，
            实际上不需要保留那么高的精度，可以通过量化，
            比如用0~255表示原来32个bit所表示的精度，
            通过牺牲精度来降低每一个权值所需要占用的空间。
    4、神经网络二值化：比量化更为极致的做法就是神经网络二值化，
                     也即将所有的权值不用浮点数表示了，
                     用二进制的数表示，要么是+1,要么是-1，用二进制的方式表示，
                     原来一个32bit权值现在只需要一个bit就可以表示，
                     可以大大减小模型尺寸。


## Deep Compression 方法，包含
    裁剪，
    量化，
    编码 三个手段。

## 模型参数分析：
    网络中全连层参数和卷积层weight占绝大多数，
    卷积层的bias只占极小部分。
    而参数分布在0附近，近似高斯分布。
    参数压缩针对卷积层的weight和全连层参数。每一层的参数单独压缩。
    
### 1. 剪枝(pruning)
[Lecun老爷子的OBD可以将网络中不重要的参数剔除](http://papers.nips.cc/paper/250-optimal-brain-damage.pdf)

    模型的裁剪方法则比较简单明了，直接在原有的模型上剔除掉不重要的filter，
    虽然这种压缩方式比较粗糙，但是神经网络的自适应能力很强，
    加上大的模型往往冗余比较多，将一些参数剔除之后，
    通过一些retraining的手段可以将由剔除参数而降低的性能恢复回来，
    因此只需要挑选一种合适的裁剪手段以及retraining方式，
    就能够有效的在已有模型的基础上对其进行很大程度的压缩，是目前使用最普遍的方法。
    

    基于模型裁剪的方法: 
    
           对以训练好的模型进行裁剪的方法，是目前模型压缩中使用最多的方法，
           通常是寻找一种有效的评判手段，来判断参数的重要性，
           将不重要的connection或者filter进行裁剪来减少模型的冗余。

**a. 阈值剪裁**
[基于模型裁剪的方法](https://blog.csdn.net/wspba/article/details/75675554)

    pruning可以分为三步： 
    step1. 正常训练模型得到网络权值； 
    step2. 将所有低于一定阈值的权值设为0； 
    step3. 重新训练网络中剩下的非零权值。 
    
    截止滤波后的稀疏矩阵：
![](https://img-blog.csdn.net/20161026171518038)
    
    0很多，直接存储矩阵太浪费，采用CSR 存储方式
    记录非零值 以及非零值的 索引，
    
    CSR可以将原始矩阵表达为三部分，即AA,JA,IC 
![](https://img-blog.csdn.net/20161026171745170)

    AA是矩阵A中所有非零元素，长度为a，即非零元素个数； 
    JA是矩阵A中每行第一个非零元素在AA中的位置，最后一个元素是非零元素数加1，长度为n+1, n是矩阵A的行数；  
    IC是AA中每个元素对应的列号，长度为a。 
    所以将一个稀疏矩阵转为CSR表示，需要的空间为2*a+n+1个，同理CSC也是类似。 

[caffe-python 剪枝 实例](https://github.com/Ewenwan/Caffe-Python-Tutorial/blob/master/prune.py)


**b.基于量级的裁剪方式**
[相关论文](https://arxiv.org/pdf/1608.08710.pdf)

    基于量级的裁剪方式，用weight值的大小来评判其重要性，
    对于一个filter，其中所有weight的绝对值求和，后通过排序来区分重要性。
    来作为该filter的评价指标，
    将一层中值低的filter裁掉，
    可以有效的降低模型的复杂度并且不会给模型的性能带来很大的损失.
    https://arxiv.org/pdf/1608.08710.pdf
    
    作者在裁剪的时候同样会考虑每一层对裁剪的敏感程度，作者会单独裁剪每一层来看裁剪后的准确率。
    对于裁剪较敏感的层，作者使用更小的裁剪力度，或者跳过这些层不进行裁剪。
    目前这种方法是实现起来较为简单的，并且也是非常有效的，
    它的思路非常简单，就是认为参数越小则越不重要。

**c.统计filter中激活为0的值的数量作为标准**
[论文参考](https://arxiv.org/pdf/1607.03250.pdf)

    作者认为，在大型的深度学习网络中，大部分的神经元的激活都是趋向于零的，
    而这些激活为0的神经元是冗余的，将它们剔除可以大大降低模型的大小和运算量，
    而不会对模型的性能造成影响，于是作者定义了一个量APoZ（Average Percentage of Zeros）
    来衡量每一个filter中激活为0的值的数量，来作为评价一个filter是否重要的标准。
    
**d.基于熵值的剪裁**
[论文参考](https://arxiv.org/pdf/1706.05791.pdf)

    作者认为通过weight值的大小很难判定filter的重要性，
    通过这个来裁剪的话有可能裁掉一些有用的filter。
    因此作者提出了一种基于熵值的裁剪方式，利用熵值来判定filter的重要性。 
    作者将每一层的输出通过一个Global average Pooling,
    将feature map 转换为一个长度为c（filter数量）的向量，
    对于n张图像可以得到一个n*c的矩阵，
    对于每一个filter，将它分为m个bin，统计每个bin的概率pi，
    然后计算它的熵值,利用熵值来判定filter的重要性，再对不重要的filter进行裁剪。
    第j个feature map熵值的计算方式如下： 
    
    Hj = -sum(pi*log(pi))
    
    在retrain中，作者使用了这样的策略，即每裁剪完一层，通过少数几个迭代来恢复部分的性能，
    当所有层都裁剪完之后，再通过较多的迭代来恢复整体的性能，
    作者提出，在每一层裁剪过后只使用很少的训练步骤来恢复性能，
    能够有效的避免模型进入到局部最优。

**e.基于能量效率的裁剪方式**
[论文参考](https://arxiv.org/pdf/1611.05128.pdf)
    
    作者认为以往的裁剪方法，都没有考虑到模型的带宽以及能量的消耗，
    因此无法从能量利用率上最大限度的裁剪模型，因此提出了一种基于能量效率的裁剪方式。
    作者指出一个模型中的能量消耗包含两个部分，一部分是计算的能耗，一部分是数据转移的能耗，
    在作者之前的一片论文中（与NVIDIA合作，Eyeriss），提出了一种估计硬件能耗的工具，
    能够对模型的每一层计算它们的能量消耗。然后将每一层的能量消耗从大到小排序，
    对能耗大的层优先进行裁剪，这样能够最大限度的降低模型的能耗，对于需要裁剪的层，
    根据weight的大小来选择不重要的进行裁剪，同样的作者也考虑到不正确的裁剪，
    因此将裁剪后模型损失最大的weight保留下来。 
    
    每裁剪完一层后，对于该层进行locally的fine-tune，locally的fine-tune，
    是在每一层的filter上，使用最小二乘优化的方法来使裁剪后的filter
    调整到使得输出与原始输出尽可能的接近。在所有层都裁剪完毕后，
    再通过一个global的finetuning来恢复整体的性能。
    
**f.遗传算法思想,随机剪裁，选择效果好的**
[论文参考](https://openreview.net/pdf?id=HkvS3Mqxe)
    
    作者认为，既然我无法直观上的判定filter的重要性，
    那么就采取一种随机裁剪的方式，然后对于每一种随机方式统计模型的性能，来确定局部最优的裁剪方式。 
    这种随机裁剪方式类似于一个随机mask，假设有M个潜在的可裁剪weight，那么一共就有2^M个随机mask。
    假设裁剪比例为a，那么每层就会随机选取ML*a个filter，一共随机选取N组组合，
    然后对于这N组组合，统计裁剪掉它们之后模型的性能，
    然后选取性能最高的那组作为局部最优的裁剪方式。

**g.基于icc组内相关来衡量filter的重要性**
[论文参考](https://arxiv.org/pdf/1704.06305.pdf)

    作者发现，在最后一个卷积层中，经过LDA分析发现对于每一个类别，
    有很多filter之间的激活是高度不相关的，
    因此可以利用这点来剔除大量的只具有少量信息的filter而不影响模型的性能。 
    作者在VGG-16上进行实验，VGG-16的conv5_3具有512个filter，
    将每一个filter的输出值中的最大值定义为该filter的fire score，
    因此对应于每一张图片就具有一个512维的fire向量，当输入一堆图片时，
    就可以得到一个N*512的fire矩阵，作者用intra-class correlation来衡量filter的重要性： 
    
    作者这样做的目的是通过只保留对分类任务提取特征判别性最强的filter，来降低模型的冗余。


**h.基于神经元激活相关性的重要性判别方法**
[论文参考](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Sun_Sparsifying_Neural_Network_CVPR_2016_paper.pdf)

    作者认为，如果一层中的某个神经元的激活与上一层的某个神经元的激活有很强的相关性，
    那么这个神经元对于后面层的激活具有很强的判别性。
    也就是说，如果前后两层中的某对神经元的激活具有较高的相关性，
    那么它们之间的连接weight就是非常重要的，而弱的相关性则代表低的重要性。
    如果某个神经元可以视为某个特定视觉模式的探测器，那么与它正相关的神经元也提供了这个视觉模式的信息，
    而与它负相关的神经元则帮助减少误报。作者还认为，那些相关性很低的神经元对，
    它们之间的连接不一定是一点用也没有，它们可能是对于高相关性神经元对的补充。
    
**i.将裁剪问题当做一个组合优化问题**
[论文参考](https://arxiv.org/pdf/1611.06440.pdf)

    作者将裁剪问题当做一个组合优化问题：从众多的权重参数中选择一个最优的组合B，使得被裁剪的模型的代价函数的损失最小.
    这类似于Oracle pruning的方式，即通过将每一个weight单独的剔除后看模型损失函数的衰减，
    将衰减最少的参数认为是不重要的参数，可以剔除，这也是OBD的思路，但是OBD的方法需要求二阶导数，
    实现起来难度较大，而本文提出的Taylor expansion的方法可以很好的解决这个问题.
    
**j. 一种基于Hessian矩阵的网络修剪算法**    
[论文参考](https://papers.nips.cc/paper/647-second-order-derivatives-for-network-pruning-optimal-brain-surgeon.pdf)

[参考博文](https://blog.csdn.net/qq_19645269/article/details/78791652)

    OBS算法是一种基于Hessian矩阵的网络修剪算法，首先，构造误差曲面的一个局部模型，分析权值的扰动所造成的影响。 
    通过对误差函数进行Taylor展开
    可以得到 与二阶导数 海塞矩阵H相关的一个式子
    
![](https://img-blog.csdn.net/20171213154134181?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMTk2NDUyNjk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

    1.OBS算法的全称为optimal brain surgeon,翻译成中文就是最优外科手术，表面的意思就是该方法是和神经网络过程是分开的。 
    2.该方法是一种框架，只要是模型能求出参数的梯度，那么都可用这个方法进行稀疏化。 


### 2. 量化(Quantization)
    为了进一步压缩网络，考虑让若干个权值共享同一个权值，
    这一需要存储的数据量也大大减少。
    在论文中，采用kmeans算法来将权值进行聚类，
    在每一个类中，所有的权值共享该类的聚类质心，
    因此最终存储的结果就是一个码书和索引表。
    
    1.对权值聚类 
        论文中采用kmeans聚类算法，
        通过优化所有类内元素到聚类中心的差距（within-cluster sum of squares ）来确定最终的聚类结果.
        
    2. 聚类中心初始化 

        常用的初始化方式包括3种： 
        a) 随机初始化。
           即从原始数据种随机产生k个观察值作为聚类中心。 

        b) 密度分布初始化。
           现将累计概率密度CDF的y值分布线性划分，
           然后根据每个划分点的y值找到与CDF曲线的交点，再找到该交点对应的x轴坐标，将其作为初始聚类中心。 

        c) 线性初始化。
            将原始数据的最小值到最大值之间的线性划分作为初始聚类中心。 

        三种初始化方式的示意图如下所示： 

![](https://img-blog.csdn.net/20161026183710142)

    由于大权值比小权值更重要（参加HanSong15年论文），
    而线性初始化方式则能更好地保留大权值中心，
    因此文中采用这一方式，
    后面的实验结果也验证了这个结论。 
    
    3. 前向反馈和后项传播 
        前向时需要将每个权值用其对应的聚类中心代替，
        后向计算每个类内的权值梯度，
        然后将其梯度和反传，
        用来更新聚类中心，
        如图： 
        
![](https://img-blog.csdn.net/20161026184233327)

        共享权值后，就可以用一个码书和对应的index来表征。
        假设原始权值用32bit浮点型表示，量化区间为256，
        即8bit，共有n个权值，量化后需要存储n个8bit索引和256个聚类中心值，
        则可以计算出压缩率compression ratio: 
            r = 32*n / (8*n + 256*32 )≈4 
            可以看出，如果采用8bit编码，则至少能达到4倍压缩率。

### 3. 编码(Huffman Encoding)

### 4. 迁移学习方法 基于教师——学生网络的方法

    基于教师——学生网络的方法，属于迁移学习的一种。
    迁移学习也就是将一个模型的性能迁移到另一个模型上，
    而对于教师——学生网络，教师网络往往是一个更加复杂的网络，
    具有非常好的性能和泛化能力，
    可以用这个网络来作为一个soft target来指导另外一个更加简单的学生网络来学习，
    使得更加简单、参数运算量更少的学生模型也能够具有和教师网络相近的性能，
    也算是一种模型压缩的方式。
    
**a. Distilling the Knowledge in a Neural Network**
[论文参考](https://arxiv.org/pdf/1503.02531.pdf)

    较大、较复杂的网络虽然通常具有很好的性能，
    但是也存在很多的冗余信息，因此运算量以及资源的消耗都非常多。
    而所谓的Distilling就是将复杂网络中的有用信息提取出来迁移到一个更小的网络上，
    这样学习来的小网络可以具备和大的复杂网络想接近的性能效果，并且也大大的节省了计算资源。
    这个复杂的网络可以看成一个教师，而小的网络则可以看成是一个学生。 
 
    这个复杂的网络是提前训练好具有很好性能的网络，
    学生网络的训练含有两个目标：
    一个是hard target，即原始的目标函数，为小模型的类别概率输出与label真值的交叉熵；
    另一个为soft target，为小模型的类别概率输出与大模型的类别概率输出的交叉熵.
    在soft target中，概率输出的公式调整如下，
    这样当T值很大时，可以产生一个类别概率分布较缓和的输出
    
    作者认为，由于soft target具有更高的熵，它能比hard target提供更加多的信息，
    因此可以使用较少的数据以及较大的学习率。
    将hard和soft的target通过加权平均来作为学生网络的目标函数，
    soft target所占的权重更大一些。 
    作者同时还指出，T值取一个中间值时，效果更好，
    而soft target所分配的权重应该为T^2，hard target的权重为1。 
    这样训练得到的小模型也就具有与复杂模型近似的性能效果，但是复杂度和计算量却要小很多。
    
    对于distilling而言，复杂模型的作用事实上是为了提高label包含的信息量。
    通过这种方法，可以把模型压缩到一个非常小的规模。
    模型压缩对模型的准确率没有造成太大影响，而且还可以应付部分信息缺失的情况。
    
    
**b.使用复杂网络中能够提供视觉相关位置信息的Attention map来监督小网络的学习**
[论文参考](https://arxiv.org/pdf/1612.03928.pdf)  
[代码](https://github.com/szagoruyko/attention-transfer)

    作者借鉴Distilling的思想，
    使用复杂网络中能够提供视觉相关位置信息的Attention map来监督小网络的学习，
    并且结合了低、中、高三个层次的特征.
    
    教师网络从三个层次的Attention Transfer对学生网络进行监督。
    其中三个层次对应了ResNet中三组Residual Block的输出。
    在其他网络中可以借鉴。 
    这三个层次的Attention Transfer基于Activation，
    Activation Attention为feature map在各个通道上的值求和，
    
    但是就需要两次反向传播的过程，实现起来较困难并且效果提升不明显。
    基于Activation的Attention Transfer效果较好，而且可以和Hinton的Distilling结合。 


# 模型压缩总结

    1. 核参数稀疏
       在损失函数中添加使得参数趋向于稀疏的项，
    使得模型在训练过程中，其参数权重趋向于稀疏。

    2. 权重矩阵低秩分解
       核心思想就是把较大的卷积核分解为两个级联的行卷积核和列卷积核，
       例如 3*3卷积分成 1*3卷积和 3*1卷积 级联。
       这里对于1*1的卷积核无效。

    3. 剪枝
       可分为在filter级别上剪枝或者在参数级别上剪枝：
       a. 对于单个filter，有阈值剪枝方法，将filter变得稀疏。
       b. 宏观上使用一种评价方法(能量大小)来计算每个filter的重要性得分，
          去除重要性低的filter。

    4. 量化
       a. 降低数据数值范围
          单精度浮点数（32）-> 半精度浮点数（16）
          ->无符号(8) -> 三值 -> 二值
       b. 聚类编码实现权值共享的方法
          对卷积核参数进行k_means聚类，得到聚类中心。，
          对原始参数，使用其所属的中心id来代替。
          然后通过储存这k个类别的中心值，以及id局矩阵来压缩网络。
          
    5. 迁移学习
       通过将较大较复杂较优秀的网络(老师)中的有用信息提取出来迁移到一个更小的网络上(学生)，
       这样学习来的小网络可以具备和大的复杂网络相想接近的性能效果，实现网络的压缩。


