# 性能提升方法
    1. 小模型 mobilenet
    2. 模型压缩：参数稀疏、量化、分解、去冗余。
    3. 高性能计算。

# 模型压缩

[DeepCompression-caffe](https://github.com/Ewenwan/DeepCompression-caffe)


## 为什么要压缩网络？
    做过深度学习的应该都知道，NN大法确实效果很赞，
    在各个领域轻松碾压传统算法，
    不过真正用到实际项目中却会有很大的问题：

    计算量非常巨大；
    模型特别吃内存；
    
    这两个原因，使得很难把NN大法应用到嵌入式系统中去，
    因为嵌入式系统资源有限，而NN模型动不动就好几百兆。
    所以，计算量和内存的问题是作者的motivation；

## 如何压缩？
      论文题目已经一句话概括了：
        Prunes the network：只保留一些重要的连接；
        Quantize the weights：通过权值量化来共享一些weights；
        Huffman coding：通过霍夫曼编码进一步压缩；
        
## 效果如何？
    Pruning：把连接数减少到原来的 1/13~1/9； 
    Quantization：每一个连接从原来的 32bits 减少到 5bits；

## 最终效果： 
    - 把AlextNet压缩了35倍，从 240MB，减小到 6.9MB； 
    - 把VGG-16压缩了49北，从 552MB 减小到 11.3MB； 
    - 计算速度是原来的3~4倍，能源消耗是原来的3~7倍；

## 网络压缩(network compression)
    尽管深度神经网络取得了优异的性能，
    但巨大的计算和存储开销成为其部署在实际应用中的挑战。
    有研究表明，神经网络中的参数存在大量的冗余。
    因此，有许多工作致力于在保证准确率的同时降低网路复杂度。

### 1、低秩近似。
     用低秩矩阵近似原有权重矩阵。
     例如，可以用SVD得到原矩阵的最优低秩近似，
     或用Toeplitz矩阵配合Krylov分解近似原矩阵。

### 2、剪枝(pruning) 在训练结束后，可以将一些不重要的神经元连接
    (可用权重数值大小衡量配合损失函数中的稀疏约束)或整个滤波器去除，
    之后进行若干轮微调。实际运行中，神经元连接级别的剪枝会
    使结果变得稀疏，
    不利于缓存优化和内存访问，有的需要专门设计配套的运行库。
    相比之下，滤波器级别的剪枝可直接运行在现有的运行库下，
    而滤波器级别的剪枝的关键是如何衡量滤波器的重要程度。
    例如，可用卷积结果的稀疏程度、该滤波器对损失函数的影响、
    或卷积结果对下一层结果的影响来衡量。
### 3、量化(quantization)。对权重数值进行聚类，
    用聚类中心数值代替原权重数值，配合Huffman编码，
    具体可包括标量量化或乘积量化。
    但如果只考虑权重自身，容易造成量化误差很低，
    但分类误差很高的情况。
    因此，Quantized CNN优化目标是重构误差最小化。
    此外，可以利用哈希进行编码，
    即被映射到同一个哈希桶中的权重共享同一个参数值。

### 4、降低数据数值范围。
    默认情况下数据是单精度浮点数，占32位。
    有研究发现，改用半精度浮点数(16位)
    几乎不会影响性能。谷歌TPU使用8位整型来
    表示数据。极端情况是数值范围为二值
    或三值(0/1或-1/0/1)，
    这样仅用位运算即可快速完成所有计算，
    但如何对二值或三值网络进行训练是一个关键。
    通常做法是网络前馈过程为二值或三值，
    梯度更新过程为实数值。

## 已训练好的模型上做裁剪
    这种就是在训练好的模型上做一些修改，
    然后在fine-tuning到原来的准确率，
    主要有一些方法：
    1、剪枝：神经网络是由一层一层的节点通过边连接，每个边上会有权重，
            所谓剪枝，就是当我们发现某些边上的权重很小，
            可以认为这样的边不重要，进而可以去掉这些边。
            在训练的过程中，在训练完大模型之后，
            看看哪些边的权值比较小，把这些边去掉，然后继续训练模型；
    2、权值共享：就是让一些边共用一个权值，达到缩减参数个数的目的。
                假设相邻两层之间是全连接，每层有1000个节点，
                那么这两层之间就有1000*1000=100万个权重参数。
                可以将这一百万个权值做聚类，利用每一类的均值代替这一类中的每个权值大小，
                这样同属于一类的很多边共享相同的权值，假设把一百万个权值聚成一千类，则可以把参数个数从一百万降到一千个。
    3、量化：一般而言，神经网络模型的参数都是用的32bit长度的浮点型数表示，
            实际上不需要保留那么高的精度，可以通过量化，
            比如用0~255表示原来32个bit所表示的精度，
            通过牺牲精度来降低每一个权值所需要占用的空间。
    4、神经网络二值化：比量化更为极致的做法就是神经网络二值化，
                     也即将所有的权值不用浮点数表示了，
                     用二进制的数表示，要么是+1,要么是-1，用二进制的方式表示，
                     原来一个32bit权值现在只需要一个bit就可以表示，
                     可以大大减小模型尺寸。


## Deep Compression 方法，包含
    裁剪，
    量化，
    编码 三个手段。

## 模型参数分析：
    网络中全连层参数和卷积层weight占绝大多数，
    卷积层的bias只占极小部分。
    而参数分布在0附近，近似高斯分布。




参数压缩针对卷积层的weight和全连层参数。
每一层的参数单独压缩。
