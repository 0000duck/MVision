# 深度学习结合SLAM 研究现状总结
## 用深度学习方法替换传统slam中的一个/几个模块： 
                特征提取，特征匹配，提高特征点稳定性，提取点线面等不同层级的特征点。
                深度估计
                位姿估计
                重定位
                其他
        目前还不能达到超越传统方法的效果，
        相较传统SLAM并没有很明显的优势（标注的数据集少且不全，使用视频做训练数据的非常少。
        SLAM中很多问题都是数学问题，深度学习并不擅长等等原因）。

## 在传统SLAM之上加入语义信息 
                图像语义分割
                语义地图构建
        语义SLAM算是在扩展了传统SLAM问题的研究内容，现在出现了一些将语义信息集成到SLAM的研究，
        比如说用SLAM系统中得到的图像之间的几何一致性促进图像语义分割，
        也可以用语义分割/建图的结果促进SLAM的定位/闭环等，前者已经有了一些研究，
        不过还是集中于室内场景，后者貌似还没有什么相关研究。
        如果SLAM和语义分割能够相互促进相辅相成，应该能达到好的效果。
        另：使用SLAM帮助构建大规模的图像之间有对应关系的数据集，
        可以降低深度学习数据集的标注难度吧，应该也是一个SLAM助力深度学习的思路。

## 端到端SLAM
        其实端到端就不能算是SLAM问题了吧，SLAM是同步定位与地图构建，端到端是输入image输出action，没有定位和建图。 
        - 机器人自主导航（深度强化学习）等


# Vision Semantic SLAM  视觉分割SLAM   语义SLAM
        SLAM的另一个大方向就是和深度学习技术结合。
        到目前为止，SLAM的方案都处于特征点或者像素的层级。
        关于这些特征点或像素到底来自于什么东西，我们一无所知。
        这使得计算机视觉中的SLAM与我们人类的做法不怎么相似，
        至少我们自己从来看不到特征点，也不会去根据特征点判断自身的运动方向。
        我们看到的是一个个物体，通过左右眼判断它们的远近，
        然后基于它们在图像当中的运动推测相机的移动。

        很久之前，研究者就试图将物体信息结合到SLAM中。
        例如文献[135-138]中就曾把物体识别与视觉SLAM结合起来，构建带物体标签的地图。
        另一方面，把标签信息引入到BA或优化端的目标函数和约束中，
        我们可以结合特征点的位置与标签信息进行优化。
        这些工作都可以称为语义SLAM。

        语义和SLAM看似是两个独立的模块，实则不然。
        在很多应用中，二者相辅相成。
        一方面，语义信息可以帮助SLAM提高建图和定位的精度，特别是对于复杂的动态场景。
        传统SLAM的建图和定位多是基于像素级别的几何匹配。
        借助语义信息，我们可以将数据关联从传统的像素级别升级到物体级别，提升复杂场景下的精度。
        另一方面，借助SLAM技术计算出物体之间的位置约束，
        可以对同一物体在不同角度，
        不同时刻的识别结果进行一致性约束，从而提高语义理解的精度。

        综合来说，SLAM和语义的结合点主要有两个方面：

        1、 语义帮助SLAM。
          传统的物体识别、分割算法往往只考虑一幅图，
          而在SLAM中我们拥有一台移动的相机。
          如果我们把运动过程中的图片都带上物体标签，就能得到一个带有标签的地图。
          另外，物体信息亦可为回环检测、BA优化带来更多的条件。
        2、 SLAM帮助语义。
          物体识别和分割都需要大量的训练数据。
          要让分类器识别各个角度的物体，需要从不同视角采集该物体的数据，然后进行人工标定，非常辛苦。
          而SLAM中，由于我们可以估计相机的运动，可以自动地计算物体在图像中的位置，节省人工标定的成本。
          如果有自动生成的带高质量标注的样本数据，能够很大程度上加速分类器的训练过程。

        在深度学习广泛应用之前，我们只能利用支持向量机、条件随机场等传统工具对物体或场景进行分割和识别
        或者直接将观测数据与数据库中的样本进行比较[108,140]，尝试构建语义地图[138,141-143]。
        由于这些工具本身在分类正确率上存在限制，所以效果也往往不尽如人意。
        随着深度学习的发展，我们开始使用网络，越来越准确地对图像进行识别、检测和分割[144-149]。
        这为构建准确的语义地图打下了更好的基础[150]。我们正看到，逐渐开始有学者将神经网络方法引入到SLAM中的物体识别和分割，
        甚至SLAM本身的位姿估计与回环检测中[151-153]。
        虽然这些方法目前还没有成为主流，但将SLAM与深度学习结合来处理图像，亦是一个很有前景的研究方向。

        语义slam应用:
        基本框架图如下： 
                输入RGB-D图像 -> ORB-SLAM2应用于每一帧->
        SSD（Single Shot MultiBox Detector）用于每一个关键帧进行目标检测，3D无监督分割方法对于每一个检测结果生成一个3D点云分割 ->         使用类似ICP的匹配值方法进行数据关联，以决定是否在地图中创建新的对象或者跟已有对象建立检测上的关联 -> 
        地图对象的3D模型（3D点云分割，指向ORB-SLAM2中位姿图的指针，对每个类别的累计置信度）
        
![](http://7xk58v.com2.z0.glb.qiniucdn.com/mmbiz_png/rkVzuT6J81HscekG0icQeoCsFictCol1PzexlS9rGT3NhJTkZGwbd1QX6hFTUM1SoxjF5YxmQXoyQALM1E3Msq0g/0?wx_fmt=png)


# orbslam2 + ssd物体检测实现3d物体分割
![](http://5b0988e595225.cdn.sohucs.com/images/20171130/4f7de56d3e374e13935d3a2601ccbdd2.jpeg)

        利用现在检测速度很快的SSD，以及基本上可以达到实时定位的ORB-SLAM2相互促进。
        然后通过把深度图进行划分，物体检测，最终输出带有语义信息的语义地图。
        个人觉得，本文的难点在数据的融合，也就是流程图的第三部分，两者之间怎么去相互配合。

        其次本文作者比较谦虚的承认：
        为什么没有取名为语义SLAM，是因为该工作只是用SLAM促进分割，并没有用语义分割去促进定位。
        实际上两者应该是相辅相成的，只不过貌似后者还没有人做罢了。

        本文在实时性和计算量应该算是比较合理的，一般GPU应该能跑起来；缺点是地图可读性比较差。

[Meaningful Maps With Object-Oriented Semantic Mapping](https://arxiv.org/pdf/1609.07849.pdf)

# 单目 LSD-SLAM + CNN卷积网络物体分割 

        基本框架图如下： 
        输入RGB图像->选择关键帧并refine->2D语义分割->3D重建，语义优化 
        
![](http://5b0988e595225.cdn.sohucs.com/images/20171130/7f16a83b8526473c8165206c1596c4b0.jpeg)

        东南大学的一个同学做的，算是参考文献就6页的神文，CVPR2016最佳论文之一。
[论文](https://arxiv.org/pdf/1611.04144v1.pdf)

        大致思路：
                利用LSD-SLAM作为框架，结合CNN进行有机融合，
                选择关键帧进行做深度学习实现语义分割，之后选择相邻的几帧做增强。



