#!/usr/bin/env python
#-*- coding:utf-8 -*-
# 多类别逻辑回归
# 手写字体MNIST 识别 逻辑回归
# 多类模型跟线性回归的主要区别在于输出节点从一个变成了多个
# 简单逻辑回归模型  y=a*X + b
from mxnet import ndarray as nd
from mxnet import autograd
import random
from mxnet import gluon

import matplotlib.pyplot as plt#画图

import mxnet as mx


##########################################################
#### 准备输入数据 ###
mnist = mx.test_utils.get_mnist()
# (batch_size, num_channels, width, height) 这里为灰度图像num_channels=1  width=height=28
batch_size = 100#每次训练载入 100张图片
train_iter = mx.io.NDArrayIter(mnist['train_data'], mnist['train_label'], batch_size, shuffle=True)# 随机打乱 shuffle=True
val_iter = mx.io.NDArrayIter(mnist['test_data'], mnist['test_label'], batch_size)

###########################################################
### 定义模型 ##################
#@@@@初始化模型参数 权重和偏置@@@@
num_inputs = 784#输入数据维度  1*784
num_outputs = 10#输出标签维度 1*10
W = nd.random_normal(shape=(num_inputs, num_outputs))
 # 256*784  ×  784*10 ——————> 256*10 每张图像预测10个类别的概率 每次共256张图片
b = nd.random_normal(shape=num_outputs)#偏置
params = [W, b]#权重和偏置 参数
 # 模型参数附上梯度 自动微分时 会计算梯度信息
for param in params:
    param.attach_grad()

#@@@@@定义模型@@@@@@
## softmax 回归实现  exp(Xi)/(sum(exp(Xi))) 归一化概率 使得 10类概率之和为1
def softmax(X):
    exp = nd.exp(X)
    # 假设exp是矩阵，这里对行进行求和，并要求保留axis 1，
    # 就是返回 (nrows, 1) 形状的矩阵
    partition = exp.sum(axis=1, keepdims=True)
    return exp / partition
## 测试softmax
# 我们将每个元素变成了非负数，而且每一行加起来为1
X = nd.random_normal(shape=(2,5))
X_prob = softmax(X)
print(X_prob)
print(X_prob.sum(axis=1))#按行求和

## 定义模型
def net(X):
    return softmax(nd.dot(X.reshape((-1,num_inputs)), W) + b)# y = softmax( a*X + b ) 

#############################################
### 损失函数
# 交叉熵损失函数 
# 我们需要定义一个针对预测为概率值的损失函数
# 它将两个概率分布的负交叉熵作为目标值，最小化这个值等价于最大化这两个概率的相似度
# 就是 使得 预测的 1*10的概率分布  尽可能 和 标签 1*10的概率分布相等  减小两个概率分布间 的混乱度(熵)
# 具体 真实标签 y=1 , y_lab=[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]  
# 那么交叉熵就是y_lab[0]*log(y_pre[0])+...+y_lab[n]*log(y_pre[n])
# 注意到yvec里面只有一个1，那么前面等价于log(y_pre[y])

def cross_entropy(yhat, y):#yhat为预测 y为真实标签
    return - nd.pick(nd.log(yhat), y)#注意为 负交叉熵

##################################################
##### 准确度
# 我们将预测概率最高的那个类作为预测的类，然后通过比较真实标号我们可以计算精度
def accuracy(output, label):#预测输出 output 真实标签label
    return nd.mean(output.argmax(axis=1)==label).asscalar()
# 评估数据集上的准确度
def evaluate_accuracy(data_iterator, net):
    acc = 0.
    for data, label in data_iterator:# 数据集里的 样本和标签
        output = net(data)#网络输出
        acc += accuracy(output, label)
    return acc / len(data_iterator)#平均准确度
#因为我们随机初始化了模型，所以这个模型的精度应该大概是1/num_outputs = 0.1
print evaluate_accuracy(test_data, net)

###############################################
### 优化模型 #################################
# 这里通过随机梯度下降来求解 
# 模型参数沿着梯度的反方向走特定距离，这个距离一般叫学习率
def SGD(params, lr):
    for param in params:
        param[:] = param - lr * param.grad#更新每个参数

#############################################
### 训练 #######################
learning_rate = .1#学习率
epochs = 7##训练迭代 次数
for epoch in range(epochs):
    train_loss = 0.# 损失
    train_acc = 0. #准确度
    for data, label in train_data:#训练数据集 样本和标签
        with autograd.record():#自动微分
            output = net(data) #网络输出
            loss = cross_entropy(output, label)##损失
        loss.backward()#向后传播
        # 将梯度做平均，这样学习率会对batch size不那么敏感
        SGD(params, learning_rate/batch_size)

        train_loss += nd.mean(loss).asscalar()#损失
        train_acc += accuracy(output, label)  #准确度

    test_acc = evaluate_accuracy(test_data, net)#验证数据集的准确度
    print("训练次数 %d. 损失Loss: %f, 训练准确度Train acc %f, 测试准确度Test acc %f" % (
        epoch, train_loss/len(train_data), train_acc/len(train_data), test_acc))


## 查看训练结果

data, label = mnist_test[0:10]#测试数据集前10个数据
show_images(data)#图片实例
print('true labels')#真实标签
print(get_text_labels(label))

predicted_labels = net(data).argmax(axis=1)#将预测概率最高的那个类作为预测的类
print('predicted labels')#预测标签 
print(get_text_labels(predicted_labels.asnumpy()))





